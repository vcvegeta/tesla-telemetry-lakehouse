name: tesla-telemetry

x-airflow-common: &airflow-common
  image: apache/airflow:2.9.3
  environment: &airflow-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    AIRFLOW__CORE__FERNET_KEY: ""
    AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
  volumes:
    - ../airflow/dags:/opt/airflow/dags
    - ../airflow/logs:/opt/airflow/logs
    - ../airflow/plugins:/opt/airflow/plugins
    - /var/run/docker.sock:/var/run/docker.sock
  user: "0:0"
  depends_on:
    - postgres

services:
  postgres:
    image: postgres:16
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db migrate &&
        airflow users create \
          --username admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com \
          --password admin
    restart: "no"

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8089:8080"
    restart: always

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    restart: always

  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.1
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.6.1
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181

      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:9092,EXTERNAL://localhost:9093
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL

      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    ports:
      - "9093:9093"


  spark-master:
    build:
      context: .
      dockerfile: spark/Dockerfile
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master"]
    environment:
      AWS_ACCESS_KEY_ID: minio
      AWS_SECRET_ACCESS_KEY: minio12345
      AWS_REGION: us-east-1
      S3_ENDPOINT: http://minio:9000
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - ../spark:/opt/spark/work-dir/spark

  spark-worker:
    build:
      context: .
      dockerfile: spark/Dockerfile
    depends_on:
      - spark-master
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]
    environment:
      AWS_ACCESS_KEY_ID: minio
      AWS_SECRET_ACCESS_KEY: minio12345
      AWS_REGION: us-east-1
      S3_ENDPOINT: http://minio:9000
    ports:
      - "8081:8081"
    volumes:
      - ../spark:/opt/spark/work-dir/spark
  
  spark-worker-2:
    build:
      context: .
      dockerfile: spark/Dockerfile
    depends_on:
      - spark-master
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]
    environment:
      AWS_ACCESS_KEY_ID: minio
      AWS_SECRET_ACCESS_KEY: minio12345
      AWS_REGION: us-east-1
      S3_ENDPOINT: http://minio:9000
    ports:
      - "8082:8081"
    volumes:
      - ./spark:/opt/spark/work-dir/spark

  # Dedicated Spark cluster for batch jobs
  spark-master-batch:
    build:
      context: .
      dockerfile: spark/Dockerfile
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master"]
    environment:
      AWS_ACCESS_KEY_ID: minio
      AWS_SECRET_ACCESS_KEY: minio12345
      AWS_REGION: us-east-1
      S3_ENDPOINT: http://minio:9000
    ports:
      - "7078:7077"
      - "8083:8080"
    volumes:
      - ../spark:/opt/spark/work-dir/spark

  spark-worker-batch:
    build:
      context: .
      dockerfile: spark/Dockerfile
    depends_on:
      - spark-master-batch
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master-batch:7077"]
    environment:
      AWS_ACCESS_KEY_ID: minio
      AWS_SECRET_ACCESS_KEY: minio12345
      AWS_REGION: us-east-1
      S3_ENDPOINT: http://minio:9000
    ports:
      - "8084:8081"
    volumes:
      - ../spark:/opt/spark/work-dir/spark

  minio:
    image: minio/minio:RELEASE.2024-12-18T13-15-44Z
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio12345
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data

  superset:
    build:
      context: ./superset
    ports:
      - "8088:8088"
    environment:
      SUPERSET_SECRET_KEY: "change_me_please"
    depends_on:
      - postgres
  
  ingestor:
    build:
      context: ../services/ingestor
    env_file:
      - ../services/ingestor/.env
    depends_on:
      - kafka
    restart: always

  streaming-kafka-to-bronze:
    build:
      context: .
      dockerfile: spark/Dockerfile
    depends_on:
      - spark-master
      - kafka
    user: "0:0"
    environment:
      AWS_ACCESS_KEY_ID: minio
      AWS_SECRET_ACCESS_KEY: minio12345
      AWS_REGION: us-east-1
      S3_ENDPOINT: http://minio:9000
    command: >
      /opt/spark/bin/spark-submit
      --master spark://spark-master:7077
      --deploy-mode client
      --driver-memory 1g
      --executor-memory 1g
      --executor-cores 1
      --total-executor-cores 2
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,org.apache.hadoop:hadoop-aws:3.3.4
      /opt/spark/work-dir/spark/streaming_jobs/kafka_to_minio_bronze.py
    volumes:
      - ../spark:/opt/spark/work-dir/spark
      - spark_ivy_cache:/home/spark/.ivy2
    restart: always

  streaming-bronze-to-silver:
    build:
      context: .
      dockerfile: spark/Dockerfile
    depends_on:
      - spark-master
      - streaming-kafka-to-bronze
    user: "0:0"
    environment:
      AWS_ACCESS_KEY_ID: minio
      AWS_SECRET_ACCESS_KEY: minio12345
      AWS_REGION: us-east-1
      S3_ENDPOINT: http://minio:9000
    command: >
      /opt/spark/bin/spark-submit
      --master spark://spark-master:7077
      --deploy-mode client
      --driver-memory 1g
      --executor-memory 1g
      --executor-cores 1
      --total-executor-cores 2
      --packages org.apache.hadoop:hadoop-aws:3.3.4
      /opt/spark/work-dir/spark/streaming_jobs/bronze_to_silver.py
    volumes:
      - ../spark:/opt/spark/work-dir/spark
      - spark_ivy_cache:/home/spark/.ivy2
    restart: always

volumes:
  postgres_data:
  minio_data:
  spark_ivy_cache:
